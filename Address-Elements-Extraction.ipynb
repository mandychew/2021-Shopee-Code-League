{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qy8a-PihvKbw",
    "outputId": "547efc9f-f9f5-44de-8d94-5f257f1b6deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras. preprocessing.sequence import pad_sequences\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "20Ojl6SbvNox"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1vfS8IVvO9d",
    "outputId": "7445f3fb-4abb-4617-f8aa-7c64f90bc174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                        raw_address  \\\n",
      "0   0  jl kapuk timur delta sili iii lippo cika 11 a ...   \n",
      "1   1                                 aye, jati sampurna   \n",
      "2   2               setu siung 119 rt 5 1 13880 cipayung   \n",
      "3   3                               toko dita, kertosono   \n",
      "4   4                                      jl. orde baru   \n",
      "\n",
      "                                  POI/street  \n",
      "0  /jl kapuk timur delta sili iii lippo cika  \n",
      "1                                          /  \n",
      "2                                     /siung  \n",
      "3                                 toko dita/  \n",
      "4                             /jl. orde baru  \n",
      "   id                                    raw_address\n",
      "0   0          s. par 53 sidanegara 4 cilacap tengah\n",
      "1   1          angg per, baloi indah kel. lubuk baja\n",
      "2   2                          asma laun, mand imog,\n",
      "3   3  ud agung rej, raya nga sri wedari karanganyar\n",
      "4   4                     cut mutia, 35 baiturrahman\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('C:/Users/mandy/OneDrive/Documents/GitHub/2021-Shopee-Code-League_Address-Elements-Extraction/test.csv')\n",
    "train = pd.read_csv('C:/Users/mandy/OneDrive/Documents/GitHub/2021-Shopee-Code-League_Address-Elements-Extraction/train.csv')\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBWqs9Fz22Rm",
    "outputId": "4a2e86fa-f929-4750-927a-0f308c40b999"
   },
   "outputs": [],
   "source": [
    "print(train['raw_address'].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0R78VFYgPwiC"
   },
   "outputs": [],
   "source": [
    "# creating regex to split raw_address into words\n",
    "my_regex = '\\s|,\\s|,|\\.|:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2i0EufN7pj1"
   },
   "source": [
    "# POI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf_4uc0MyTNg"
   },
   "source": [
    "## Splitting Data - POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "6-t6WRONvWNO",
    "outputId": "e8fe28c9-4388-4425-c54c-3de08bb7dd54"
   },
   "outputs": [],
   "source": [
    "raw_df_withPOI = train[train['POI/street'].str.contains('.+(?=/)')==True]\n",
    "raw_df_withPOI['POI/street'] = raw_df_withPOI['POI/street'].str.extract(r'(.+)(?=/)')\n",
    "raw_df_withPOI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIqC0ChT2-sV",
    "outputId": "7659442c-c951-43f7-dcd0-a26bafdc17ff"
   },
   "outputs": [],
   "source": [
    "# turning raw_address and POI/street columns into lists of strings; discarding id and index column\n",
    "list_of_addresses_withPOI = [*raw_df_withPOI['raw_address']]\n",
    "print(list_of_addresses_withPOI[:5])\n",
    "\n",
    "list_of_POI = [*raw_df_withPOI['POI/street']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-tk-DNh0Aqy"
   },
   "source": [
    "## Creating labels_binary_POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4juDoGQhnwx"
   },
   "outputs": [],
   "source": [
    "max_length_address = 25   #len(max(raw_df_withPOI['raw_address']))\n",
    "max_length_POI = 10       #len(max(raw_df_withPOI['POI/street']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOcANrD99Iuq"
   },
   "outputs": [],
   "source": [
    "dictionary_POI = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5V3YV6lAfNx",
    "outputId": "4b0031ff-7354-43ba-d450-1aa310e83cf9"
   },
   "outputs": [],
   "source": [
    "# binary labels for training set\n",
    "number_of_binary_labels = len(list_of_addresses_withPOI)\n",
    "print(len(list_of_addresses_withPOI))\n",
    "labels_binary_POI = [[]]*number_of_binary_labels\n",
    "\n",
    "\n",
    "# for each word in 'raw_address', match it to words in 'POI/street' column.\n",
    "# if there's a match, value 1; if no match, value 0\n",
    "for i in range(number_of_binary_labels):\n",
    "  label_binary = [0 for _ in range(max_length_address)]\n",
    "  words = re.split(my_regex, list_of_POI[i])\n",
    "  address = re.split(my_regex, list_of_addresses_withPOI[i])\n",
    "\n",
    "  for word in words:\n",
    "    for j, ad in enumerate(address):\n",
    "      if word.startswith(ad):\n",
    "        if (ad != word) & ((word in dictionary_POI.values())==False):\n",
    "          dictionary_POI[ad] = word\n",
    "        if j<max_length_address:\n",
    "          label_binary[j] = 1\n",
    "          break\n",
    "\n",
    "  labels_binary_POI[i] = label_binary\n",
    " \n",
    "labels_binary_POI = np.array(labels_binary_POI)\n",
    "print(labels_binary_POI[:5])\n",
    "print(dict(list(dictionary_POI.items())[0:5]))\n",
    "print(len(dictionary_POI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbbvOUtg9KRc",
    "outputId": "0a62927b-fa79-4e49-9f4b-5c76a152f857"
   },
   "outputs": [],
   "source": [
    "print(len(dictionary_POI))\n",
    "\n",
    "def filter_dict(mydict):\n",
    "    filter_dict_num = {k: mydict[k] for k in sorted(mydict.keys()) if not any(map(str.isdigit, k))}\n",
    "    filter_dict_len = {k: filter_dict_num[k] for k in sorted(filter_dict_num.keys()) if len(k)>0}\n",
    "    return filter_dict_len\n",
    "\n",
    "filtered_dict_POI = filter_dict(dictionary_POI)\n",
    "print(len(filtered_dict_POI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjVObcvDpucA"
   },
   "source": [
    "## Tokenizer and Padding - POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67ZRLvHgvc4v"
   },
   "outputs": [],
   "source": [
    "# # plotting distribution of length of raw_addresses and of POI\n",
    "# length_of_addresses = [len(re.split('\\s|, |\\.|:', address)) for address in train_addresses_withPOI]\n",
    "# print(length_of_addresses[:5])\n",
    "# print(max(length_of_addresses))\n",
    "# print(len(train_addresses_withPOI))\n",
    "\n",
    "# A = collections.Counter(length_of_addresses)\n",
    "# plt.bar(A.keys(), A.values())\n",
    "# plt.show()\n",
    "\n",
    "# length_of_POI = [len(re.split('\\s|, |\\.|:', POI)) for POI in train_labels_POI]\n",
    "# print(length_of_POI[:5])\n",
    "# print(max(length_of_POI))\n",
    "\n",
    "# P = collections.Counter(length_of_POI)\n",
    "# plt.bar(P.keys(), P.values())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDZnYOoc8sRA",
    "outputId": "9db694a2-5996-4825-cc00-30c878621e8c"
   },
   "outputs": [],
   "source": [
    "# vocab_size = 200000\n",
    "embedding_dim = 64\n",
    "trunc_type = 'post'\n",
    "padding = 'post'\n",
    "oov_tok=\"<OOV>\"\n",
    "\n",
    "print(type(list_of_addresses_withPOI))\n",
    "sentences_to_tokenize = list_of_addresses_withPOI.copy()\n",
    "\n",
    "print(type(sentences_to_tokenize))\n",
    "\n",
    "print(len(list_of_addresses_withPOI))\n",
    "\n",
    "sentences_to_tokenize.append(list_of_POI)\n",
    "print(len(list_of_addresses_withPOI))\n",
    "print(sentences_to_tokenize[:5])\n",
    "\n",
    "\n",
    "# sentences_to_tokenize = train_addresses.append(train_POI_sentences)\n",
    "# print(sentences_to_tokenize[:5])\n",
    "# for row in train_labels_POI:\n",
    "#   train_addresses.append(row)\n",
    "# print(sentences_to_tokenize[-5:])\n",
    "\n",
    "\n",
    "# for row in test_labels_POI:\n",
    "#   test_sentences.append(row)\n",
    "# print(test_sentences[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHIjhRL_Cxud"
   },
   "outputs": [],
   "source": [
    "if len(list_of_addresses_withPOI) != len(list_of_POI):\n",
    "  print(len(list_of_addresses_withPOI))\n",
    "  print(len(list_of_POI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9A8y3dxfp-Ac",
    "outputId": "70cbb10d-6827-4b77-bdcd-725af6e48e89"
   },
   "outputs": [],
   "source": [
    "# Tokenizing train_addresses and train_POI\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences_to_tokenize)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "total_words = len(word_index)+1\n",
    "\n",
    "def tokenize_and_pad(data):\n",
    "  data_sequences = tokenizer.texts_to_sequences(data)\n",
    "  data_padded = pad_sequences(data_sequences, maxlen=max_length_address, truncating=trunc_type, padding=padding)\n",
    "  return data_padded\n",
    "\n",
    "#Padding addresses\n",
    "padded = tokenize_and_pad(list_of_addresses_withPOI)\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqzL6YDDDuBX",
    "outputId": "180cd7d6-90fe-441f-d621-1b192d0a2048"
   },
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[1]))\n",
    "print(list_of_addresses_withPOI[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZyj0sSoq4mO"
   },
   "source": [
    "# Subword Tokenization\n",
    "too advanced, skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiTDdhCpizUh"
   },
   "outputs": [],
   "source": [
    "# #!pip install -q tensorflow_text_nightly\n",
    "# !pip install -q tf-nightly\n",
    "# !pip install tokenizers -q\n",
    "\n",
    "# from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "# from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0ETrhvQqOGh"
   },
   "outputs": [],
   "source": [
    "# # Initialize an empty BERT tokenizer\n",
    "# tokenizer = BertWordPieceTokenizer(\n",
    "#   clean_text=False,\n",
    "#   handle_chinese_chars=False,\n",
    "#   strip_accents=False,\n",
    "#   lowercase=True,\n",
    "# )\n",
    "\n",
    "# # prepare text files to train vocab on them\n",
    "# test_string = ['/content/test.txt']\n",
    "# #[sentences_to_tokenize]\n",
    "# #\n",
    "\n",
    "# # train BERT tokenizer\n",
    "# tokenizer.train(\n",
    "#   test_string,\n",
    "#   vocab_size=20,\n",
    "#   min_frequency=2,\n",
    "#   show_progress=True,\n",
    "#   special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\n",
    "#   limit_alphabet=1000,\n",
    "#   wordpieces_prefix=\"##\"\n",
    "# )\n",
    "\n",
    "# # save the vocab\n",
    "# tokenizer.save('/content/bert-vocab.txt')\n",
    "\n",
    "# # create a BERT tokenizer with trained vocab\n",
    "# vocab = '/content/bert-vocab.txt'\n",
    "# tokenizer = BertWordPieceTokenizer(vocab)\n",
    "\n",
    "# # test the tokenizer with some text\n",
    "# encoded = tokenizer.encode('...')\n",
    "# print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xj1FVOLsjrsX"
   },
   "outputs": [],
   "source": [
    "# #bert_tokenizer_params=dict(lower_case=True)\n",
    "# reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "# bert_vocab_args = dict(\n",
    "#     # The target vocabulary size\n",
    "#     vocab_size = 97192,\n",
    "#     # Reserved tokens that must be included in the vocabulary\n",
    "#     reserved_tokens=reserved_tokens,\n",
    "#     # Arguments for `text.BertTokenizer`\n",
    "#     bert_tokenizer_params=dict(lower_case=True),\n",
    "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "#     learn_params={},\n",
    "# )\n",
    "\n",
    "# vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#     sentences_to_tokenize, \n",
    "#     **bert_vocab_args\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AMfR1g4IeMY"
   },
   "outputs": [],
   "source": [
    "# Tokenize corpus (all words AI should learn) without vocab_size limit \n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(sentences_to_tokenize)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# print(len(word_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHfIizvJ_oxP"
   },
   "source": [
    "## Train Test Split - POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uXJE8p9lDEM",
    "outputId": "50de8174-255a-4d97-91d8-e42fd2d819bd"
   },
   "outputs": [],
   "source": [
    "# train_test_split to get training and testing datasets\n",
    "train_addresses_withPOI, test_addresses_withPOI, train_labels_POI, test_labels_POI = train_test_split(\n",
    "padded, labels_binary_POI, test_size=0.20, random_state=42)\n",
    "\n",
    "# print(len(padded))\n",
    "# print(len(train_labels_POI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9wKiwgM5RDi"
   },
   "source": [
    "## Model building - POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLUTJQV7shx1",
    "outputId": "4a080c71-a3d6-4ffb-9bb7-87c7707c5ce6"
   },
   "outputs": [],
   "source": [
    "# basic model - no Bidirectional, LSTM, etc.\n",
    "model_POI = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length=max_length_address),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(max_length_address, activation='sigmoid')\n",
    "])\n",
    "model_POI.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_POI.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90CG89TwwqYx"
   },
   "outputs": [],
   "source": [
    "if train_padded.shape[0] != labels_binary_POI.shape[0]:\n",
    "  print('Training addresses and labels have different number of rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MlLXUQqsyb1",
    "outputId": "07ef41e9-b259-4f7a-a314-129153445898"
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor = 'val_acc', patience = 5)\n",
    "modelCheckpoint = ModelCheckpoint('best_model.hdf5', save_best_only = True)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "history = model_POI.fit(train_addresses_withPOI, train_labels_POI, epochs=num_epochs, validation_data=(test_addresses_withPOI, test_labels_POI), callbacks=[earlyStopping, modelCheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5VNnmRAtq7E"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQJKhS0TBRR3"
   },
   "source": [
    "## model_POI.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFWykKvMcKdA"
   },
   "outputs": [],
   "source": [
    "print(type(test))\n",
    "live_padded = tokenize_and_pad(test['raw_address'])\n",
    "print(live_padded.shape)\n",
    "\n",
    "# live_to_predict = test_padded[:5]\n",
    "# live_prediction_POI = model_POI.predict(live_to_predict)\n",
    "\n",
    "def predict_and_convert_to_words(live_to_predict, raw_address, threshold, dictionary, model):\n",
    "  live_prediction = model.predict(live_to_predict)\n",
    "  number_of_live_labels = len(live_to_predict)\n",
    "  # print(number_of_live_labels)\n",
    "\n",
    "  live_labels = [[]]*number_of_live_labels\n",
    "  \n",
    "  # decode live_labels to regular words\n",
    "  for i in range(number_of_live_labels):\n",
    "    live_label = ''\n",
    "    address = re.split(my_regex, raw_address.iloc[i])\n",
    "  \n",
    "    for id, j in enumerate(live_prediction[i]):\n",
    "      if (j > threshold) & (id < len(address)):\n",
    "        if address[id] in dictionary:\n",
    "          # print(id, j, address[id])\n",
    "          new_string = dictionary.get(address[id])\n",
    "          live_label += new_string\n",
    "          live_label += ' '\n",
    "        else:\n",
    "          # print(id, j, address[id])\n",
    "          live_label += address[id]\n",
    "          live_label += ' '\n",
    "    # print(live_label_POI)\n",
    "    live_label = live_label.rstrip()\n",
    "    live_labels[i] = live_label\n",
    "  return live_labels\n",
    "\n",
    "live_labels_POI = predict_and_convert_to_words(live_padded, test['raw_address'], threshold=0.5, filtered_dict_POI, model_POI)\n",
    "print(live_labels_POI[:5])\n",
    "print(test['raw_address'][:5])\n",
    "\n",
    "# if want to run test, will get error as train_addresses_withPOI is not a DataFrame\n",
    "# test = predict_and_convert_to_words(test_padded[:5], train_addresses_withPOI, filtered_dict_POI, model_POI)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV-NqI4p7ecv"
   },
   "source": [
    "# Street (ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI1Kqvvf8Y1h"
   },
   "source": [
    "## Splitting Data - ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ne42fKQ8Y1i"
   },
   "outputs": [],
   "source": [
    "raw_df_withST = train[train['POI/street'].str.contains('(?<=/).+')==True]\n",
    "raw_df_withST['POI/street'] = raw_df_withST['POI/street'].str.extract(r'(?<=/)(.+)')\n",
    "raw_df_withST.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9mu80ml8Y1k"
   },
   "outputs": [],
   "source": [
    "# turning raw_address and POI/street columns into lists of strings; discarding id and index column\n",
    "list_of_addresses_withST = [*raw_df_withST['raw_address']]\n",
    "print(list_of_addresses_withST[:5])\n",
    "\n",
    "list_of_ST = [*raw_df_withST['POI/street']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM0QM8QG8Y1m"
   },
   "source": [
    "## Creating labels_binary_ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayajg7mQ8Y1n"
   },
   "outputs": [],
   "source": [
    "# plotting distribution of length of raw_addresses and of POI\n",
    "length_of_addresses_ST = [len(re.split('\\s|, |\\.|:', address)) for address in list_of_addresses_withST]\n",
    "print(length_of_addresses_ST[:5])\n",
    "print(max(length_of_addresses_ST))\n",
    "\n",
    "A = collections.Counter(length_of_addresses_ST)\n",
    "plt.bar(A.keys(), A.values())\n",
    "plt.show()\n",
    "\n",
    "length_of_ST = [len(re.split('\\s|, |\\.|:', ST)) for ST in list_of_ST]\n",
    "print(length_of_ST[:5])\n",
    "print(max(length_of_ST))\n",
    "\n",
    "P = collections.Counter(length_of_ST)\n",
    "plt.bar(P.keys(), P.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO2dhgKo8Y1n"
   },
   "outputs": [],
   "source": [
    "max_length_address = 25   #len(max(raw_df_withST['raw_address']))\n",
    "max_length_ST = 10       #len(max(raw_df_withST['POI/street']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyVZVFrW1F9V"
   },
   "outputs": [],
   "source": [
    "dictionary_ST = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smiaI31m8Y1n"
   },
   "outputs": [],
   "source": [
    "# binary labels for training set\n",
    "number_of_binary_labels = len(list_of_addresses_withST)\n",
    "print(len(list_of_addresses_withST))\n",
    "labels_binary_ST = [[]]*number_of_binary_labels\n",
    "\n",
    "\n",
    "# for each word in 'raw_address', match it to words in 'POI/street' column.\n",
    "# if there's a match, value 1; if no match, value 0\n",
    "for i in range(number_of_binary_labels):\n",
    "  label_binary = [0 for _ in range(max_length_address)]\n",
    "  words = re.split(my_regex, list_of_ST[i])\n",
    "  address = re.split(my_regex, list_of_addresses_withST[i])\n",
    "\n",
    "  for word in words:\n",
    "    for j, ad in enumerate(address):\n",
    "      if word.startswith(ad):\n",
    "        if (ad != word) & ((word in dictionary_ST.values())==False):\n",
    "          dictionary_ST[ad] = word\n",
    "        if j<max_length_address:\n",
    "          label_binary[j] = 1\n",
    "          break\n",
    "\n",
    "  labels_binary_ST[i] = label_binary\n",
    " \n",
    "labels_binary_ST = np.array(labels_binary_ST)\n",
    "print(labels_binary_ST[:5])\n",
    "print(list_of_addresses_withST[:5])\n",
    "print(list_of_ST[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U26UHwl00rI"
   },
   "outputs": [],
   "source": [
    "print(len(dictionary_ST))\n",
    "\n",
    "# def filter_dict(mydict):\n",
    "#     filter_dict_num = {k: mydict[k] for k in sorted(mydict.keys()) if not any(map(str.isdigit, k))}\n",
    "#     filter_dict_len = {k: filter_dict_num[k] for k in sorted(filter_dict_num.keys()) if len(k)>0}\n",
    "#     return filter_dict_len\n",
    "\n",
    "filtered_dict_ST = filter_dict(dictionary_ST)\n",
    "print(len(filtered_dict_ST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQVcfp-z8Y1p"
   },
   "source": [
    "## Tokenizer and Padding - ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKdVSlOi8Y1p"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "trunc_type = 'post'\n",
    "padding = 'post'\n",
    "oov_tok=\"<OOV>\"\n",
    "\n",
    "print(type(list_of_addresses_withST))\n",
    "sentences_to_tokenize = list_of_addresses_withST.copy()\n",
    "\n",
    "print(type(sentences_to_tokenize))\n",
    "# train_ST_sentences = []\n",
    "\n",
    "print(len(list_of_addresses_withST))\n",
    "\n",
    "sentences_to_tokenize.append(list_of_ST)\n",
    "print(len(list_of_addresses_withST))\n",
    "print(sentences_to_tokenize[:5])\n",
    "\n",
    "\n",
    "# sentences_to_tokenize = train_addresses.append(train_ST_sentences)\n",
    "# print(sentences_to_tokenize[:5])\n",
    "# for row in list_of_ST:\n",
    "#   train_addresses.append(row)\n",
    "# print(sentences_to_tokenize[-5:])\n",
    "\n",
    "\n",
    "# for row in test_labels_ST:\n",
    "#   test_sentences.append(row)\n",
    "# print(test_sentences[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHIjhRL_Cxud"
   },
   "outputs": [],
   "source": [
    "if len(list_of_addresses_withST) != len(list_of_ST):\n",
    "  print(len(list_of_addresses_withPOI))\n",
    "  print(len(list_of_POI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoxtJjDN8Y1q"
   },
   "outputs": [],
   "source": [
    "# Tokenizing train_addresses and train_ST\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences_to_tokenize)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "total_words = len(word_index)+1\n",
    "\n",
    "#Padding train_addresses, train_ST, test_addresses\n",
    "padded = tokenize_and_pad(list_of_addresses_withST)\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkzOTU3F8Y1r"
   },
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(train_padded[1]))\n",
    "print(list_of_addresses_withST[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_O1Tsxg8Y1k"
   },
   "source": [
    "## Train Test Split - ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZUioROF8Y1l"
   },
   "outputs": [],
   "source": [
    "# train_test_split to get training and testing datasets\n",
    "train_addresses_withST, test_addresses_withST, train_labels_ST, test_labels_ST = train_test_split(\n",
    "padded, labels_binary_ST, test_size=0.20, random_state=42)\n",
    "\n",
    "print(len(train_addresses_withST))\n",
    "print(len(train_labels_ST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8RcL-298Y1r"
   },
   "source": [
    "## Model building - ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ3u89xQ8Y1r"
   },
   "outputs": [],
   "source": [
    "# basic model - no Bidirectional, LSTM, etc.\n",
    "model_ST = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length=max_length_address),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(max_length_address, activation='sigmoid')\n",
    "])\n",
    "model_ST.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_ST.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj9QWmFJ8Y1r"
   },
   "outputs": [],
   "source": [
    "if train_padded.shape[0] != labels_binary_ST.shape[0]:\n",
    "  print('Training addresses and labels have different number of rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCKhNcpv8Y1t"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "history = model_ST.fit(train_addresses_withST, train_labels_ST, epochs=num_epochs, validation_data=(test_addresses_withST, test_labels_ST), callbacks=[earlyStopping, modelCheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA-JvzKK8Y1t"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9No9Hf7o0qD"
   },
   "source": [
    "## model_ST.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRLmAh460eOQ"
   },
   "outputs": [],
   "source": [
    "live_padded_ST = tokenize_and_pad(test['raw_address'])\n",
    "print(live_padded_ST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAl-CtwHHmZM"
   },
   "outputs": [],
   "source": [
    "live_labels_ST = predict_and_convert_to_words(live_padded_ST, test['raw_address'], threshold=0.5, filtered_dict_ST, model_ST)\n",
    "print(live_labels_ST[:5])\n",
    "print(test['raw_address'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45fctQmwYheo"
   },
   "source": [
    "# Combine results of POI and ST - POI/Street column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3Zf-1ZVYgnY"
   },
   "outputs": [],
   "source": [
    "print('live_labels_ST is type: ' + str(type(live_labels_ST)))\n",
    "print('live_labels_POI is type: ' + str(type(live_labels_POI)))\n",
    "\n",
    "# check if live_labels_ST and _POI have same number of elements\n",
    "if len(live_labels_ST) != len(live_labels_POI):\n",
    "  print('live_labels_ST and _POI have different number of elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdPGUfWqSIdJ"
   },
   "outputs": [],
   "source": [
    "test['POI'] = live_labels_POI\n",
    "test['Street'] = live_labels_ST\n",
    "\n",
    "print(test.head())\n",
    "\n",
    "test['POI/Street'] = test.POI.cat(test.Street, sep='/')\n",
    "print(test.head())\n",
    "\n",
    "submission = test[['id', 'POI/Street']]\n",
    "print(submission)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mandy Data Science - Shopee Code League.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
