{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mandy Data Science - Shopee Code League.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy8a-PihvKbw",
        "outputId": "547efc9f-f9f5-44de-8d94-5f257f1b6deb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras. preprocessing.sequence import pad_sequences\n",
        "print(tf.__version__)\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Ojl6SbvNox"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import collections"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1vfS8IVvO9d",
        "outputId": "7445f3fb-4abb-4617-f8aa-7c64f90bc174"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "try:\n",
        "  test = pd.read_csv('/content/drive/MyDrive/Data Science/test.csv')\n",
        "  train = pd.read_csv('/content/drive/MyDrive/Data Science/train.csv')\n",
        "except:\n",
        "  test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/scl-2021-ds/test.csv')\n",
        "  train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/scl-2021-ds/train.csv')\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   id  ...                                 POI/street\n",
            "0   0  ...  /jl kapuk timur delta sili iii lippo cika\n",
            "1   1  ...                                          /\n",
            "2   2  ...                                     /siung\n",
            "3   3  ...                                 toko dita/\n",
            "4   4  ...                             /jl. orde baru\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "   id                                    raw_address\n",
            "0   0          s. par 53 sidanegara 4 cilacap tengah\n",
            "1   1          angg per, baloi indah kel. lubuk baja\n",
            "2   2                          asma laun, mand imog,\n",
            "3   3  ud agung rej, raya nga sri wedari karanganyar\n",
            "4   4                     cut mutia, 35 baiturrahman\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBWqs9Fz22Rm",
        "outputId": "4a2e86fa-f929-4750-927a-0f308c40b999"
      },
      "source": [
        "print(train['raw_address'].sample(20))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "296515    tegal gundil pami 5, 15 rt 7 6 bogor utara - kota\n",
            "67561             kali baru timur #, kali baru medan satria\n",
            "81292                       sun angit 156 30752 babat toman\n",
            "82194     perumahan wahana cikarang, blok b 2 no 25 suka...\n",
            "225176      rio motor rs fatma, cipete utara kebayoran baru\n",
            "77752                       pem poris plawad utara cipondoh\n",
            "1868        raya cibu bog 392 gunungbatu bogor barat - kota\n",
            "31007                    klinik al-i, raya bayongbong ciela\n",
            "19669                                         rth bajulmati\n",
            "240438                             sumur gayam, kapas kapas\n",
            "49689                      k 33 ulujami rt 9 4 pesanggrahan\n",
            "55227     globe surya udara pt, benda barat 2, kaliabang...\n",
            "38219                    tpa an nur, kemuningsari lor panti\n",
            "288403        pengadegan pan sela vii 28-30 rt 7 5 pancoran\n",
            "216335    tamalanrea jaya peri kemerd viii bg.91 rt 5 rw...\n",
            "111617    tanjung duren selatan gg. iii 8 rt 3 2 grogol ...\n",
            "151639           mushallina tai, raya jatiwar jati waringin\n",
            "85864         bidan agustinar p, fajar niaga, jaka sampurna\n",
            "114541      cam gg. ii no 22 semper barat rt 7 14 cilincing\n",
            "242818              gg. man kedin lor 8-10 1 60129 kenjeran\n",
            "Name: raw_address, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R78VFYgPwiC"
      },
      "source": [
        "# creating regex to split raw_address into words\n",
        "my_regex = '\\s|,\\s|,|\\.|:'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2i0EufN7pj1"
      },
      "source": [
        "# POI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_4uc0MyTNg"
      },
      "source": [
        "## Splitting Data - POI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "6-t6WRONvWNO",
        "outputId": "e8fe28c9-4388-4425-c54c-3de08bb7dd54"
      },
      "source": [
        "raw_df_withPOI = train[train['POI/street'].str.contains('.+(?=/)')==True]\n",
        "raw_df_withPOI['POI/street'] = raw_df_withPOI['POI/street'].str.extract(r'(.+)(?=/)')\n",
        "raw_df_withPOI.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>raw_address</th>\n",
              "      <th>POI/street</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>toko dita, kertosono</td>\n",
              "      <td>toko dita</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>raya samb gede, 299 toko bb kids</td>\n",
              "      <td>toko bb kids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>cikahuripan sd neg boj 02 klap boj, no 5 16877</td>\n",
              "      <td>sd negeri bojong 02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>yaya atohar,</td>\n",
              "      <td>yayasan atohariyah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>kampung.gudang areng,desa:anyer, kecamatan:any...</td>\n",
              "      <td>gudang areng</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                        raw_address           POI/street\n",
              "3    3                               toko dita, kertosono            toko dita\n",
              "5    5                   raya samb gede, 299 toko bb kids         toko bb kids\n",
              "10  10     cikahuripan sd neg boj 02 klap boj, no 5 16877  sd negeri bojong 02\n",
              "11  11                                       yaya atohar,   yayasan atohariyah\n",
              "15  15  kampung.gudang areng,desa:anyer, kecamatan:any...         gudang areng"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIqC0ChT2-sV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7659442c-c951-43f7-dcd0-a26bafdc17ff"
      },
      "source": [
        "# turning raw_address and POI/street columns into lists of strings; discarding id and index column\n",
        "list_of_addresses_withPOI = [*raw_df_withPOI['raw_address']]\n",
        "print(list_of_addresses_withPOI[:5])\n",
        "\n",
        "list_of_POI = [*raw_df_withPOI['POI/street']]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['toko dita, kertosono', 'raya samb gede, 299 toko bb kids', 'cikahuripan sd neg boj 02 klap boj, no 5 16877', 'yaya atohar,', 'kampung.gudang areng,desa:anyer, kecamatan:anyar, kabupaten: serang, belakang bca anyar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHfIizvJ_oxP"
      },
      "source": [
        "## Train Test Split - POI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uXJE8p9lDEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50de8174-255a-4d97-91d8-e42fd2d819bd"
      },
      "source": [
        "# train_test_split to get training and testing datasets\n",
        "train_addresses_withPOI, test_addresses_withPOI, train_labels_POI, test_labels_POI = train_test_split(\n",
        "list_of_addresses_withPOI, list_of_POI, test_size=0.20, random_state=42)\n",
        "\n",
        "print(len(train_addresses_withPOI))\n",
        "print(len(train_labels_POI))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97192\n",
            "97192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKTMOx81g7ya",
        "outputId": "c71177e8-1c1c-4f00-e2d4-a67526002aad"
      },
      "source": [
        "print(test_labels_POI[:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['toko kelontong siantar 21', 'perumahan graha indah', 'ruko pgm', 'cluster acacia', 'perumnas benteng']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-tk-DNh0Aqy"
      },
      "source": [
        "## Creating labels_binary_POI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4juDoGQhnwx"
      },
      "source": [
        "max_length_address = 25   #len(max(raw_df_withPOI['raw_address']))\n",
        "max_length_POI = 10       #len(max(raw_df_withPOI['POI/street']))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fenf3q4EyvBP"
      },
      "source": [
        "# # testing labels_binary_POI with 5 rows of raw_df_withPOI.shape\n",
        "# labels_binary_POI = [[]]*5 # *raw_df_withPOI.shape[0]\n",
        "# print(len(raw_df_withPOI))\n",
        "# print(raw_df_withPOI.shape[0])\n",
        "\n",
        "# # for each word in 'raw_address', match it to words in 'POI/street' column.\n",
        "# # if there's a match, value 1; if no match, value 0\n",
        "# for i in range(len(raw_df_withPOI.iloc[:5])):\n",
        "#   label_binary = [0 for _ in range(23)]\n",
        "#   words = re.split(my_regex, raw_df_withPOI['POI/street'].iloc[i])\n",
        "#   address = re.split(my_regex, raw_df_withPOI['raw_address'].iloc[i])\n",
        "\n",
        "#   for word in words:\n",
        "#     for j, ad in enumerate(address):\n",
        "#       if ad in word:\n",
        "#         label_binary[j] = 1\n",
        "#         break\n",
        "\n",
        "#   labels_binary_POI[i] = label_binary\n",
        " \n",
        "# labels_binary_POI = np.array(labels_binary_POI)\n",
        "# print(labels_binary_POI[:5])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe2CKmc6kWwX"
      },
      "source": [
        "# # finding out why 5th row has anomaly / wrong answer\n",
        "# words = re.split(my_regex, raw_df_withPOI['raw_address'].iloc[4])\n",
        "# print(words[8]) # check if the 9th position is whitespace (which is indicated as '1' in binary label)\n",
        "# for word in words:\n",
        "#   print(word)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOcANrD99Iuq"
      },
      "source": [
        "dictionary_POI = {}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5V3YV6lAfNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0031ff-7354-43ba-d450-1aa310e83cf9"
      },
      "source": [
        "# binary labels for training set\n",
        "number_of_binary_labels = len(train_addresses_withPOI)\n",
        "print(len(train_addresses_withPOI))\n",
        "labels_binary_POI = [[]]*number_of_binary_labels\n",
        "\n",
        "\n",
        "# for each word in 'raw_address', match it to words in 'POI/street' column.\n",
        "# if there's a match, value 1; if no match, value 0\n",
        "for i in range(number_of_binary_labels):\n",
        "  label_binary = [0 for _ in range(max_length_address)]\n",
        "  words = re.split(my_regex, train_labels_POI[i])\n",
        "  address = re.split(my_regex, train_addresses_withPOI[i])\n",
        "\n",
        "  for word in words:\n",
        "    for j, ad in enumerate(address):\n",
        "      if word.startswith(ad):\n",
        "        if (ad != word) & ((word in dictionary_POI.values())==False):\n",
        "          dictionary_POI[ad] = word\n",
        "        if j<max_length_address:\n",
        "          label_binary[j] = 1\n",
        "          break\n",
        "\n",
        "  labels_binary_POI[i] = label_binary\n",
        " \n",
        "labels_binary_POI = np.array(labels_binary_POI)\n",
        "print(labels_binary_POI[:5])\n",
        "print(dict(list(dictionary_POI.items())[0:5]))\n",
        "print(len(dictionary_POI))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97192\n",
            "[[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "{'swa': 'swasta', 'bub': 'bubble', 'apo': 'apotek', 'lest': 'lestari', 'sen': 'sendal'}\n",
            "8945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-615ARIJDFZS",
        "outputId": "badec9a6-2caa-446a-960f-863b6df3852c"
      },
      "source": [
        "#binary labels for validation set\n",
        "number_of_binary_labels = len(test_addresses_withPOI)\n",
        "test_labels_binary_POI = [[]]*number_of_binary_labels\n",
        "\n",
        "\n",
        "# for each word in 'raw_address', match it to words in 'POI/street' column.\n",
        "# if there's a match, value 1; if no match, value 0\n",
        "for i in range(len(test_addresses_withPOI)):\n",
        "  label_binary = [0 for _ in range(max_length_address)]\n",
        "  words = re.split(my_regex, test_labels_POI[i])\n",
        "  address = re.split(my_regex, test_addresses_withPOI[i])\n",
        "\n",
        "  for word in words:\n",
        "    for j, ad in enumerate(address):\n",
        "      if word.startswith(ad):\n",
        "        if (ad != word) & ((word in dictionary_POI.values())==False):\n",
        "          dictionary_POI[ad] = word\n",
        "        if j<max_length_address:\n",
        "          label_binary[j] = 1\n",
        "          break\n",
        "\n",
        "  test_labels_binary_POI[i] = label_binary\n",
        " \n",
        "test_labels_binary_POI = np.array(test_labels_binary_POI)\n",
        "print(test_labels_binary_POI[:5])\n",
        "print(len(dictionary_POI))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "10200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbbvOUtg9KRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a62927b-fa79-4e49-9f4b-5c76a152f857"
      },
      "source": [
        "print(len(dictionary_POI))\n",
        "\n",
        "def filter_dict(mydict):\n",
        "    filter_dict_num = {k: mydict[k] for k in sorted(mydict.keys()) if not any(map(str.isdigit, k))}\n",
        "    filter_dict_len = {k: filter_dict_num[k] for k in sorted(filter_dict_num.keys()) if len(k)>0}\n",
        "    return filter_dict_len\n",
        "\n",
        "filtered_dict_POI = filter_dict(dictionary_POI)\n",
        "print(len(filtered_dict_POI))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10200\n",
            "10087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVObcvDpucA"
      },
      "source": [
        "## Tokenizer and Padding - POI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ZRLvHgvc4v"
      },
      "source": [
        "# plotting distribution of length of raw_addresses and of POI\n",
        "# length_of_addresses = [len(re.split('\\s|, |\\.|:', address)) for address in train_addresses_withPOI]\n",
        "# print(length_of_addresses[:5])\n",
        "# print(max(length_of_addresses))\n",
        "# print(len(train_addresses_withPOI))\n",
        "\n",
        "# A = collections.Counter(length_of_addresses)\n",
        "# plt.bar(A.keys(), A.values())\n",
        "# plt.show()\n",
        "\n",
        "# length_of_POI = [len(re.split('\\s|, |\\.|:', POI)) for POI in train_labels_POI]\n",
        "# print(length_of_POI[:5])\n",
        "# print(max(length_of_POI))\n",
        "\n",
        "# P = collections.Counter(length_of_POI)\n",
        "# plt.bar(P.keys(), P.values())\n",
        "# plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDZnYOoc8sRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db694a2-5996-4825-cc00-30c878621e8c"
      },
      "source": [
        "# vocab_size = 200000\n",
        "embedding_dim = 64\n",
        "trunc_type = 'post'\n",
        "padding = 'post'\n",
        "oov_tok=\"<OOV>\"\n",
        "\n",
        "print(type(train_addresses_withPOI))\n",
        "sentences_to_tokenize = train_addresses_withPOI.copy()\n",
        "\n",
        "print(type(sentences_to_tokenize))\n",
        "# train_POI_sentences = []\n",
        "\n",
        "print(len(train_addresses_withPOI))\n",
        "\n",
        "sentences_to_tokenize.append(train_labels_POI)\n",
        "print(len(train_addresses_withPOI))\n",
        "print(sentences_to_tokenize[:5])\n",
        "\n",
        "\n",
        "# sentences_to_tokenize = train_addresses.append(train_POI_sentences)\n",
        "# print(sentences_to_tokenize[:5])\n",
        "# for row in train_labels_POI:\n",
        "#   train_addresses.append(row)\n",
        "# print(sentences_to_tokenize[-5:])\n",
        "\n",
        "\n",
        "# for row in test_labels_POI:\n",
        "#   test_sentences.append(row)\n",
        "# print(test_sentences[-5:])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "97192\n",
            "97192\n",
            "['mi swa islamiyah syafiiyah,', 'es bub blend maak nyoes, salak,', 'apo pasar pagi, pami, 20 kejaksan', 'raya cibatu-li, tk islam al hikmah, cibatu', 'isur iv, raya ciwi,']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHIjhRL_Cxud"
      },
      "source": [
        "if len(train_addresses_withPOI) != len(train_labels_POI):\n",
        "  print(len(train_addresses_withPOI))\n",
        "  print(len(train_labels_POI))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A8y3dxfp-Ac",
        "outputId": "70cbb10d-6827-4b77-bdcd-725af6e48e89"
      },
      "source": [
        "# Tokenizing train_addresses and train_POI\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences_to_tokenize)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "total_words = len(word_index)+1\n",
        "\n",
        "def tokenize_and_pad(data):\n",
        "  data_sequences = tokenizer.texts_to_sequences(data)\n",
        "  data_padded = pad_sequences(data_sequences, maxlen=max_length_address, truncating=trunc_type, padding=padding)\n",
        "  return data_padded\n",
        "\n",
        "#Padding train_addresses and test_addresses\n",
        "train_padded = tokenize_and_pad(train_addresses_withPOI)\n",
        "print(train_padded[0])\n",
        "print(train_padded.shape)\n",
        "\n",
        "test_padded = tokenize_and_pad(test_addresses_withPOI)\n",
        "print(test_padded[0])\n",
        "print(test_padded.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "131657\n",
            "[  481   558  4941 33211     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0]\n",
            "(97192, 25)\n",
            "[ 915    6 1244 3111  194  234   11    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n",
            "(24299, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqzL6YDDDuBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180cd7d6-90fe-441f-d621-1b192d0a2048"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(train_padded[1]))\n",
        "print(train_addresses_withPOI[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "es bub blend maak nyoes salak ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
            "es bub blend maak nyoes, salak,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZyj0sSoq4mO"
      },
      "source": [
        "# Subword Tokenization\n",
        "too advanced, skipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiTDdhCpizUh"
      },
      "source": [
        "# #!pip install -q tensorflow_text_nightly\n",
        "# !pip install -q tf-nightly\n",
        "# !pip install tokenizers -q\n",
        "\n",
        "# from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "# from tokenizers import BertWordPieceTokenizer"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ETrhvQqOGh"
      },
      "source": [
        "# # Initialize an empty BERT tokenizer\n",
        "# tokenizer = BertWordPieceTokenizer(\n",
        "#   clean_text=False,\n",
        "#   handle_chinese_chars=False,\n",
        "#   strip_accents=False,\n",
        "#   lowercase=True,\n",
        "# )\n",
        "\n",
        "# # prepare text files to train vocab on them\n",
        "# test_string = ['/content/test.txt']\n",
        "# #[sentences_to_tokenize]\n",
        "# #\n",
        "\n",
        "# # train BERT tokenizer\n",
        "# tokenizer.train(\n",
        "#   test_string,\n",
        "#   vocab_size=20,\n",
        "#   min_frequency=2,\n",
        "#   show_progress=True,\n",
        "#   special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\n",
        "#   limit_alphabet=1000,\n",
        "#   wordpieces_prefix=\"##\"\n",
        "# )\n",
        "\n",
        "# # save the vocab\n",
        "# tokenizer.save('/content/bert-vocab.txt')\n",
        "\n",
        "# # create a BERT tokenizer with trained vocab\n",
        "# vocab = '/content/bert-vocab.txt'\n",
        "# tokenizer = BertWordPieceTokenizer(vocab)\n",
        "\n",
        "# # test the tokenizer with some text\n",
        "# encoded = tokenizer.encode('...')\n",
        "# print(encoded.tokens)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj1FVOLsjrsX"
      },
      "source": [
        "# #bert_tokenizer_params=dict(lower_case=True)\n",
        "# reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "# bert_vocab_args = dict(\n",
        "#     # The target vocabulary size\n",
        "#     vocab_size = 97192,\n",
        "#     # Reserved tokens that must be included in the vocabulary\n",
        "#     reserved_tokens=reserved_tokens,\n",
        "#     # Arguments for `text.BertTokenizer`\n",
        "#     bert_tokenizer_params=dict(lower_case=True),\n",
        "#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "#     learn_params={},\n",
        "# )\n",
        "\n",
        "# vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "#     sentences_to_tokenize, \n",
        "#     **bert_vocab_args\n",
        "# )\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMfR1g4IeMY"
      },
      "source": [
        "# Tokenize corpus (all words AI should learn) without vocab_size limit \n",
        "\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(sentences_to_tokenize)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "# print(len(word_index))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9wKiwgM5RDi"
      },
      "source": [
        "## Model building - POI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLUTJQV7shx1",
        "outputId": "4a080c71-a3d6-4ffb-9bb7-87c7707c5ce6"
      },
      "source": [
        "# basic model - no Bidirectional, LSTM, etc.\n",
        "model_POI = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length=max_length_address),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(max_length_address, activation='sigmoid')\n",
        "])\n",
        "model_POI.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_POI.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 25, 64)            8426112   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 25, 32)            10368     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                16640     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 25)                1625      \n",
            "=================================================================\n",
            "Total params: 8,458,905\n",
            "Trainable params: 8,458,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90CG89TwwqYx"
      },
      "source": [
        "if train_padded.shape[0] != labels_binary_POI.shape[0]:\n",
        "  print('Training addresses and labels have different number of rows')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MlLXUQqsyb1",
        "outputId": "07ef41e9-b259-4f7a-a314-129153445898"
      },
      "source": [
        "num_epochs = 3\n",
        "\n",
        "history = model_POI.fit(train_padded, labels_binary_POI, epochs=num_epochs, validation_data=(test_padded, test_labels_binary_POI))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3038/3038 [==============================] - 266s 85ms/step - loss: 0.1523 - accuracy: 0.3937 - val_loss: 0.0745 - val_accuracy: 0.4404\n",
            "Epoch 2/10\n",
            "3038/3038 [==============================] - 259s 85ms/step - loss: 0.0596 - accuracy: 0.4346 - val_loss: 0.0629 - val_accuracy: 0.4979\n",
            "Epoch 3/10\n",
            "3038/3038 [==============================] - 260s 86ms/step - loss: 0.0409 - accuracy: 0.4686 - val_loss: 0.0639 - val_accuracy: 0.4639\n",
            "Epoch 4/10\n",
            "3038/3038 [==============================] - 261s 86ms/step - loss: 0.0294 - accuracy: 0.5084 - val_loss: 0.0679 - val_accuracy: 0.4717\n",
            "Epoch 5/10\n",
            "3038/3038 [==============================] - 259s 85ms/step - loss: 0.0231 - accuracy: 0.5242 - val_loss: 0.0744 - val_accuracy: 0.5177\n",
            "Epoch 6/10\n",
            "3038/3038 [==============================] - 255s 84ms/step - loss: 0.0184 - accuracy: 0.5321 - val_loss: 0.0813 - val_accuracy: 0.6123\n",
            "Epoch 7/10\n",
            "3038/3038 [==============================] - 254s 84ms/step - loss: 0.0146 - accuracy: 0.5722 - val_loss: 0.0858 - val_accuracy: 0.5422\n",
            "Epoch 8/10\n",
            "3038/3038 [==============================] - 252s 83ms/step - loss: 0.0122 - accuracy: 0.5613 - val_loss: 0.1002 - val_accuracy: 0.5114\n",
            "Epoch 9/10\n",
            "3038/3038 [==============================] - 255s 84ms/step - loss: 0.0102 - accuracy: 0.5651 - val_loss: 0.1075 - val_accuracy: 0.5101\n",
            "Epoch 10/10\n",
            "1394/3038 [============>.................] - ETA: 2:16 - loss: 0.0089 - accuracy: 0.5495"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5VNnmRAtq7E"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQJKhS0TBRR3"
      },
      "source": [
        "## model_POI.predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFWykKvMcKdA"
      },
      "source": [
        "print(type(test))\n",
        "live_padded = tokenize_and_pad(test['raw_address'])\n",
        "print(live_padded.shape)\n",
        "\n",
        "# live_to_predict = test_padded[:5]\n",
        "# live_prediction_POI = model_POI.predict(live_to_predict)\n",
        "\n",
        "def predict_and_convert_to_words(live_to_predict, raw_address, threshold, dictionary, model):\n",
        "  live_prediction = model.predict(live_to_predict)\n",
        "  number_of_live_labels = len(live_to_predict)\n",
        "  # print(number_of_live_labels)\n",
        "\n",
        "  live_labels = [[]]*number_of_live_labels\n",
        "  \n",
        "  # decode live_labels to regular words\n",
        "  for i in range(number_of_live_labels):\n",
        "    live_label = ''\n",
        "    address = re.split(my_regex, raw_address.iloc[i])\n",
        "  \n",
        "    for id, j in enumerate(live_prediction[i]):\n",
        "      if (j > threshold) & (id < len(address)):\n",
        "        if address[id] in dictionary:\n",
        "          # print(id, j, address[id])\n",
        "          new_string = dictionary.get(address[id])\n",
        "          live_label += new_string\n",
        "          live_label += ' '\n",
        "        else:\n",
        "          # print(id, j, address[id])\n",
        "          live_label += address[id]\n",
        "          live_label += ' '\n",
        "    # print(live_label_POI)\n",
        "    live_label = live_label.rstrip()\n",
        "    live_labels[i] = live_label\n",
        "  return live_labels\n",
        "\n",
        "live_labels_POI = predict_and_convert_to_words(live_padded, test['raw_address'], threshold=0.5, filtered_dict_POI, model_POI)\n",
        "print(live_labels_POI[:5])\n",
        "print(test['raw_address'][:5])\n",
        "\n",
        "# if want to run test, will get error as train_addresses_withPOI is not a DataFrame\n",
        "# test = predict_and_convert_to_words(test_padded[:5], train_addresses_withPOI, filtered_dict_POI, model_POI)\n",
        "# print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M91ZX7vB_KW"
      },
      "source": [
        "# print(type(test))\n",
        "# live_padded = tokenize_and_pad(test['raw_address'])\n",
        "# print(live_padded.shape)\n",
        "\n",
        "# # live_to_predict = test_padded[:5]\n",
        "# # live_prediction_POI = model_POI.predict(live_to_predict)\n",
        "\n",
        "\n",
        "# live_prediction = model_POI.predict(live_padded)\n",
        "# number_of_live_labels = len(live_padded)\n",
        "# print(number_of_live_labels)\n",
        "\n",
        "# live_labels = [[]]*number_of_live_labels\n",
        "  \n",
        "# # decode live_labels to regular words\n",
        "# for i in range(number_of_live_labels):\n",
        "#   live_label = ''\n",
        "#   address = re.split(my_regex, test['raw_address'].iloc[i])\n",
        "  \n",
        "#   for id, j in enumerate(live_prediction[i]):\n",
        "#     if (j > 0.5) & (id < len(address)):\n",
        "#       if address[id] in filtered_dict_POI:\n",
        "#         # print(id, j, address[id])\n",
        "#         new_string = filtered_dict_POI.get(address[id])\n",
        "#         live_label += new_string\n",
        "#         live_label += ' '\n",
        "#       else:\n",
        "#         # print(id, j, address[id])\n",
        "#         live_label += address[id]\n",
        "#         live_label += ' '\n",
        "#   live_label = live_label.rstrip()\n",
        "\n",
        "#   live_labels[i] = live_label\n",
        "\n",
        "\n",
        "# print(live_labels[:5])\n",
        "# print(test['raw_address'][:5])\n",
        "# print(live_prediction[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-NqI4p7ecv"
      },
      "source": [
        "# Street (ST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1Kqvvf8Y1h"
      },
      "source": [
        "## Splitting Data - ST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ne42fKQ8Y1i"
      },
      "source": [
        "raw_df_withST = train[train['POI/street'].str.contains('(?<=/).+')==True]\n",
        "raw_df_withST['POI/street'] = raw_df_withST['POI/street'].str.extract(r'(?<=/)(.+)')\n",
        "raw_df_withST.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9mu80ml8Y1k"
      },
      "source": [
        "# turning raw_address and POI/street columns into lists of strings; discarding id and index column\n",
        "list_of_addresses_withST = [*raw_df_withST['raw_address']]\n",
        "print(list_of_addresses_withST[:5])\n",
        "\n",
        "list_of_ST = [*raw_df_withST['POI/street']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_O1Tsxg8Y1k"
      },
      "source": [
        "## Train Test Split - ST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZUioROF8Y1l"
      },
      "source": [
        "# train_test_split to get training and testing datasets\n",
        "train_addresses_withST, test_addresses_withST, train_labels_ST, test_labels_ST = train_test_split(\n",
        "list_of_addresses_withST, list_of_ST, test_size=0.20, random_state=42)\n",
        "\n",
        "print(len(train_addresses_withST))\n",
        "print(len(train_labels_ST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM0QM8QG8Y1m"
      },
      "source": [
        "## Creating labels_binary_ST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayajg7mQ8Y1n"
      },
      "source": [
        "# plotting distribution of length of raw_addresses and of POI\n",
        "length_of_addresses_ST = [len(re.split('\\s|, |\\.|:', address)) for address in train_addresses_withST]\n",
        "print(length_of_addresses_ST[:5])\n",
        "print(max(length_of_addresses_ST))\n",
        "\n",
        "A = collections.Counter(length_of_addresses_ST)\n",
        "plt.bar(A.keys(), A.values())\n",
        "plt.show()\n",
        "\n",
        "length_of_ST = [len(re.split('\\s|, |\\.|:', ST)) for ST in train_labels_ST]\n",
        "print(length_of_ST[:5])\n",
        "print(max(length_of_ST))\n",
        "\n",
        "P = collections.Counter(length_of_ST)\n",
        "plt.bar(P.keys(), P.values())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO2dhgKo8Y1n"
      },
      "source": [
        "max_length_address = 25   #len(max(raw_df_withST['raw_address']))\n",
        "max_length_ST = 10       #len(max(raw_df_withST['POI/street']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyVZVFrW1F9V"
      },
      "source": [
        "dictionary_ST = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smiaI31m8Y1n"
      },
      "source": [
        "# binary labels for training set\n",
        "number_of_binary_labels = len(train_addresses_withST)\n",
        "print(len(train_addresses_withST))\n",
        "labels_binary_ST = [[]]*number_of_binary_labels\n",
        "\n",
        "\n",
        "# for each word in 'raw_address', match it to words in 'POI/street' column.\n",
        "# if there's a match, value 1; if no match, value 0\n",
        "for i in range(number_of_binary_labels):\n",
        "  label_binary = [0 for _ in range(max_length_address)]\n",
        "  words = re.split(my_regex, train_labels_ST[i])\n",
        "  address = re.split(my_regex, train_addresses_withST[i])\n",
        "\n",
        "  for word in words:\n",
        "    for j, ad in enumerate(address):\n",
        "      if word.startswith(ad):\n",
        "        if (ad != word) & ((word in dictionary_ST.values())==False):\n",
        "          dictionary_ST[ad] = word\n",
        "        if j<max_length_address:\n",
        "          label_binary[j] = 1\n",
        "          break\n",
        "\n",
        "  labels_binary_ST[i] = label_binary\n",
        " \n",
        "labels_binary_ST = np.array(labels_binary_ST)\n",
        "print(labels_binary_ST[:5])\n",
        "print(train_addresses_withST[:5])\n",
        "print(train_labels_ST[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V1lW7k88Y1p"
      },
      "source": [
        "#binary labels for validation set\n",
        "number_of_binary_labels = len(test_addresses_withST)\n",
        "test_labels_binary_ST = [[]]*number_of_binary_labels\n",
        "\n",
        "\n",
        "# for each word in 'raw_address', match it to words in 'POI/street' column.\n",
        "# if there's a match, value 1; if no match, value 0\n",
        "for i in range(len(test_addresses_withST)):\n",
        "  label_binary = [0 for _ in range(max_length_address)]\n",
        "  words = re.split(my_regex, test_labels_ST[i])\n",
        "  address = re.split(my_regex, test_addresses_withST[i])\n",
        "\n",
        "  for word in words:\n",
        "    for j, ad in enumerate(address):\n",
        "      if word.startswith(ad):\n",
        "        if (ad != word) & ((word in dictionary_ST.values())==False):\n",
        "          dictionary_ST[ad] = word\n",
        "        if j<max_length_address:\n",
        "          label_binary[j] = 1\n",
        "          break\n",
        "\n",
        "  test_labels_binary_ST[i] = label_binary\n",
        " \n",
        "test_labels_binary_ST = np.array(test_labels_binary_ST)\n",
        "print(test_labels_binary_ST[:5])\n",
        "print(len(dictionary_ST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U26UHwl00rI"
      },
      "source": [
        "print(len(dictionary_ST))\n",
        "filtered_dict_ST = filter_dict(dictionary_ST)\n",
        "print(len(filtered_dict_ST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQVcfp-z8Y1p"
      },
      "source": [
        "## Tokenizer and Padding - ST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKdVSlOi8Y1p"
      },
      "source": [
        "embedding_dim = 64\n",
        "trunc_type = 'post'\n",
        "padding = 'post'\n",
        "oov_tok=\"<OOV>\"\n",
        "\n",
        "print(type(train_addresses_withST))\n",
        "sentences_to_tokenize = train_addresses_withST.copy()\n",
        "\n",
        "print(type(sentences_to_tokenize))\n",
        "# train_ST_sentences = []\n",
        "\n",
        "print(len(train_addresses_withST))\n",
        "\n",
        "sentences_to_tokenize.append(train_labels_ST)\n",
        "print(len(train_addresses_withST))\n",
        "print(sentences_to_tokenize[:5])\n",
        "\n",
        "\n",
        "# sentences_to_tokenize = train_addresses.append(train_ST_sentences)\n",
        "# print(sentences_to_tokenize[:5])\n",
        "# for row in train_labels_ST:\n",
        "#   train_addresses.append(row)\n",
        "# print(sentences_to_tokenize[-5:])\n",
        "\n",
        "\n",
        "# for row in test_labels_ST:\n",
        "#   test_sentences.append(row)\n",
        "# print(test_sentences[-5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xopLZDSs8Y1p"
      },
      "source": [
        "if len(train_addresses_withPOI) != len(train_labels_POI):\n",
        "  print(len(train_addresses_withPOI))\n",
        "  print(len(train_labels_POI))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoxtJjDN8Y1q"
      },
      "source": [
        "# Tokenizing train_addresses and train_ST\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences_to_tokenize)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "total_words = len(word_index)+1\n",
        "\n",
        "#Padding train_addresses, train_ST, test_addresses\n",
        "train_padded = tokenize_and_pad(train_addresses_withST)\n",
        "print(train_padded[0])\n",
        "print(train_padded.shape)\n",
        "\n",
        "test_padded = tokenize_and_pad(test_addresses_withST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkzOTU3F8Y1r"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(train_padded[1]))\n",
        "print(train_addresses_withST[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8RcL-298Y1r"
      },
      "source": [
        "## Model building - ST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ3u89xQ8Y1r"
      },
      "source": [
        "# basic model - no Bidirectional, LSTM, etc.\n",
        "model_ST = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, embedding_dim, input_length=max_length_address),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(max_length_address, activation='sigmoid')\n",
        "])\n",
        "model_ST.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_ST.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj9QWmFJ8Y1r"
      },
      "source": [
        "if train_padded.shape[0] != labels_binary_ST.shape[0]:\n",
        "  print('Training addresses and labels have different number of rows')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCKhNcpv8Y1t"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "history = model_ST.fit(train_padded, labels_binary_ST, epochs=num_epochs, validation_data=(test_padded, test_labels_binary_ST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-JvzKK8Y1t"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9No9Hf7o0qD"
      },
      "source": [
        "## model_ST.predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRLmAh460eOQ"
      },
      "source": [
        "live_padded_ST = tokenize_and_pad(test['raw_address'])\n",
        "print(live_padded_ST.shape)\n",
        "\n",
        "# live_to_predict = test_padded[:5]\n",
        "# live_prediction_ST = model_ST.predict(live_to_predict)\n",
        "\n",
        "\n",
        "# number_of_live_labels = len(live_to_predict)\n",
        "# print(number_of_live_labels)\n",
        "\n",
        "# live_labels_ST = [[]]*number_of_live_labels\n",
        "\n",
        "# # decode live_labels_ST to regular words\n",
        "# for i in range(number_of_live_labels):\n",
        "#   live_label_ST = ''\n",
        "#   address = re.split(my_regex, test_addresses_withST[i])\n",
        "#   # word = re.split(my_regex, address)\n",
        "  \n",
        "#   for id, j in enumerate(live_prediction_ST[i]):\n",
        "#     if (j > 0.5) & (id <= len(address)):\n",
        "#       if address[id] in dictionary_ST:\n",
        "#         # print(id, j, address[id])\n",
        "#         new_string = dictionary_ST.get(address[id])\n",
        "#         live_label_ST += new_string\n",
        "#         live_label_ST += ' '\n",
        "#       else:\n",
        "#         # print(id, j, address[id])\n",
        "#         live_label_ST += address[id]\n",
        "#         live_label_ST += ' '\n",
        "#   # print(live_label_ST)\n",
        "\n",
        "#   live_labels_ST[i] = live_label_ST\n",
        "\n",
        "# print(live_labels_ST[:5])\n",
        "# print(test_labels_ST[:5])\n",
        "# print(test_addresses_withST[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAl-CtwHHmZM"
      },
      "source": [
        "live_labels_ST = predict_and_convert_to_words(live_padded_ST, test['raw_address'], threshold=0.5, filtered_dict_ST, model_ST)\n",
        "print(live_labels_ST[:5])\n",
        "print(test['raw_address'][:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45fctQmwYheo"
      },
      "source": [
        "# Combine results of POI and ST - POI/Street column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3Zf-1ZVYgnY"
      },
      "source": [
        "print('live_labels_ST is type: ' + str(type(live_labels_ST)))\n",
        "print('live_labels_POI is type: ' + str(type(live_labels_POI)))\n",
        "\n",
        "# check if live_labels_ST and _POI have same number of elements\n",
        "if len(live_labels_ST) != len(live_labels_POI):\n",
        "  print('live_labels_ST and _POI have different number of elements')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdPGUfWqSIdJ"
      },
      "source": [
        "test['POI'] = live_labels_POI\n",
        "test['Street'] = live_labels_ST\n",
        "\n",
        "print(test.head())\n",
        "\n",
        "test['POI/Street'] = test.POI.cat(test.Street, sep='/')\n",
        "print(test.head())\n",
        "\n",
        "submission = test[['id', 'POI/Street']]\n",
        "print(submission)\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}